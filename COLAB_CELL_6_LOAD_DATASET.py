# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COLAB CELL 6: Load Dataset from HuggingFace Hub
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Copy this entire block into Colab notebook cell 6
# Dataset is already pushed to HF Hub: asifdotpy/talentai-comprehensive-dataset
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from datasets import load_dataset
import os

print("ğŸš€ Loading TalentAI Dataset from HuggingFace Hub")
print("=" * 60)

# Configuration
HF_DATASET_REPO = "asifdotpy/talentai-comprehensive-dataset"
HF_TOKEN = os.getenv("HF_TOKEN", None)

# Load dataset from HF Hub
print(f"\nğŸ“¥ Loading from HuggingFace Hub...")
print(f"   Repository: {HF_DATASET_REPO}")

try:
    dataset_dict = load_dataset(
        HF_DATASET_REPO,
        token=HF_TOKEN,
        trust_remote_code=True
    )
    
    train_data = dataset_dict['train']
    val_data = dataset_dict['validation']
    
    print(f"\nâœ… Dataset loaded successfully!")
    print(f"   Train split: {len(train_data)} examples")
    print(f"   Validation split: {len(val_data)} examples")
    print(f"   Total: {len(train_data) + len(val_data)} examples")
    
except Exception as e:
    print(f"\nâŒ Failed to load dataset: {e}")
    print(f"\nğŸ“‹ Troubleshooting:")
    print(f"   1. Verify HF_TOKEN is set in Colab Secrets")
    print(f"   2. Token must have read access to private dataset")
    print(f"   3. Dataset must be pushed to HF Hub first")
    print(f"\n   Dataset link: https://huggingface.co/datasets/{HF_DATASET_REPO}")
    raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Format for Alpaca training
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def format_alpaca(example):
    """Format example for Alpaca instruction-response training"""
    text = f"### Instruction:\n{example['instruction']}\n\n### Response:\n{example['response']}"
    return {'text': text}

# Apply formatting
print(f"\nğŸ“ Formatting for training...")
train_dataset = train_data.map(format_alpaca)
val_dataset = val_data.map(format_alpaca)

print(f"âœ… Formatted and ready for training!")
print(f"\nğŸ“Š Sample training example:")
print(train_dataset[0]['text'][:300] + "...\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NEXT STEPS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. Verify data loaded (check sample above)
# 2. Run next cells to load model and apply LoRA
# 3. Cell 9 will train using these datasets
# 4. Training ~60-90 min on T4 GPU
# 5. Final model uploaded to: asifdotpy/vetta-granite-2b-lora-v4
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
