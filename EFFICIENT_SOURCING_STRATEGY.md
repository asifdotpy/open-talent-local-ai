# Efficient Sourcing Strategy with AI Agents Architecture

**Last Updated:** December 10, 2025  
**Architecture:** Multi-Agent + Vendor API Integration  
**Goal:** Maximize sourcing efficiency while minimizing costs and maintaining compliance

---

## ðŸ“‹ Executive Summary

**Current State:**
- âœ… 8 specialized AI agents (Ports 8090-8096)
- âœ… Redis message bus coordination
- âœ… Event-driven async architecture
- âŒ No vendor API integration (yet)
- âŒ Manual scanning only (no automated enrichment)

**Proposed Enhancement:**
Integrate vendor APIs (Proxycurl/Nubela/Google CSE) into the existing agent architecture to create a **hybrid sourcing pipeline** that combines:
1. **Free discovery** (Google X-Ray via Proactive Scanning Agent)
2. **Paid enrichment** (Vendor APIs via new Data Enrichment Agent)
3. **AI qualification** (existing Quality-Focused Agent)
4. **Smart engagement** (existing Personalized Engagement Agent)

**Result:** 10x faster sourcing, 60% cost reduction vs. competitors, enterprise-grade compliance.

---

## ðŸŽ¯ Recommended Architecture Changes

### **Phase 1: Add Data Enrichment Agent (New Agent #9)**

**Purpose:** Bridge between free search and paid vendor enrichment

**Port:** 8097  
**Topic:** `agents:enrichment`

**Responsibilities:**
1. Receive light profile URLs from Proactive Scanning Agent
2. Implement pay-to-reveal logic (check user credits)
3. Call vendor APIs (Proxycurl/Nubela) for full profile data
4. Track costs per profile ($0.02-0.04)
5. Enforce rate limits (vendor-specific)
6. Log audit trail (GDPR Article 14)
7. Publish enriched profiles to Quality-Focused Agent

**Key Functions:**
```python
async def enrich_profile(url: str, vendor: str = "nubela"):
    # Check user credits
    if not has_credits(user_id):
        raise InsufficientCreditsError()
    
    # Call vendor API
    if vendor == "proxycurl":
        profile = await call_proxycurl(url)
    elif vendor == "nubela":
        profile = await call_nubela(url)
    
    # Deduct cost
    deduct_credits(user_id, cost=0.02)
    
    # Log for compliance
    log_enrichment(url, vendor, cost, timestamp)
    
    # Publish to quality agent
    await message_bus.publish(
        topic="agents:quality",
        payload={"profile": profile, "source": vendor}
    )
    
    return profile
```

---

### **Phase 2: Enhance Proactive Scanning Agent**

**Current State:** Scans LinkedIn/GitHub/Stack Overflow (manual API tokens)

**Proposed Enhancement:** Add Google Custom Search (X-Ray) integration

**New Function:**
```python
async def search_google_xray(query: str, platform: str = "linkedin"):
    """
    Free candidate discovery using Google Custom Search
    Example: site:linkedin.com/in java developer "open to work"
    """
    search_query = f"site:{platform}.com/in {query} \"open to work\""
    
    response = await google_cse_client.search(
        q=search_query,
        cx=GOOGLE_SEARCH_ENGINE_ID,
        num=10  # Free tier: 100 queries/day
    )
    
    # Extract light profiles (name, title, URL)
    light_profiles = []
    for item in response['items']:
        light_profiles.append({
            "name": extract_name(item['title']),
            "title": extract_title(item['snippet']),
            "url": item['link'],
            "platform": platform,
            "source": "google_xray"
        })
    
    # Publish to enrichment queue (pay-to-reveal)
    await message_bus.publish(
        topic="agents:enrichment:queue",
        payload={"profiles": light_profiles, "priority": "low"}
    )
    
    return light_profiles
```

**Benefits:**
- âœ… Free candidate discovery (100 searches/day on Google free tier)
- âœ… No vendor API costs until user clicks "Reveal"
- âœ… Complies with LinkedIn TOS (using Google, not scraping)

---

### **Phase 3: Update Scout Coordinator Workflow**

**New 2-Stage Sourcing Pipeline:**

```
Stage 1: Discovery (FREE)
    â†“
Proactive Scanning Agent
    â†’ Google X-Ray Search (free, 100 queries/day)
    â†’ Returns light profile cards (name, title, URL)
    â†“
Show to User in UI
    â†’ Light cards displayed
    â†’ "Reveal Full Profile" button
    â†“
User clicks "Reveal" (triggers Stage 2)

Stage 2: Enrichment (PAID)
    â†“
Data Enrichment Agent (NEW)
    â†’ Check user credits
    â†’ Call vendor API (Proxycurl/Nubela)
    â†’ Deduct $0.02-0.04 per profile
    â†’ Log audit trail (GDPR)
    â†“
Quality-Focused Agent
    â†’ AI assessment (Genkit)
    â†’ Scoring (0-100)
    â†“
Personalized Engagement Agent
    â†’ Generate outreach
    â†’ Send email/LinkedIn InMail
    â†“
Interviewer Agent
    â†’ Schedule interview
    â†’ Conduct AI interview
```

**Scout Coordinator New Endpoints:**
```python
@app.post("/pipelines/start-discovery")
async def start_discovery_phase(request: DiscoveryRequest):
    """
    Stage 1: Free discovery using Google X-Ray
    Returns: Light profile cards (no vendor API cost)
    """
    pipeline_id = create_pipeline(request.project_id)
    
    # Publish to Proactive Scanning Agent
    await message_bus.publish(
        topic="agents:scanning",
        payload={
            "pipeline_id": pipeline_id,
            "query": request.job_description,
            "platforms": ["linkedin", "github"],
            "mode": "discovery_only"  # No enrichment yet
        }
    )
    
    return {"pipeline_id": pipeline_id, "stage": "discovery"}


@app.post("/pipelines/{pipeline_id}/reveal")
async def reveal_profiles(pipeline_id: str, profile_urls: List[str]):
    """
    Stage 2: Paid enrichment (user clicks "Reveal")
    Cost: $0.02-0.04 per profile
    """
    # Check user credits
    total_cost = len(profile_urls) * 0.02
    if not has_credits(user_id, total_cost):
        raise HTTPException(402, "Insufficient credits")
    
    # Publish to Data Enrichment Agent
    await message_bus.publish(
        topic="agents:enrichment",
        payload={
            "pipeline_id": pipeline_id,
            "profile_urls": profile_urls,
            "vendor": "nubela",  # Or "proxycurl"
            "user_id": user_id
        }
    )
    
    return {"enriching": len(profile_urls), "cost": total_cost}
```

---

## ðŸ’° Cost Optimization Strategies

### **Strategy 1: Batched Enrichment**

**Problem:** Enriching 100 profiles individually = 100 API calls = $2-4

**Solution:** Batch enrichment with priority queue

```python
# In Data Enrichment Agent
enrichment_queue = PriorityQueue()

async def batch_enrich_worker():
    """Process enrichment requests in batches"""
    while True:
        # Collect 10 profiles (or wait 30 seconds)
        batch = []
        for _ in range(10):
            try:
                profile = enrichment_queue.get(timeout=30)
                batch.append(profile)
            except QueueEmpty:
                break
        
        if batch:
            # Call vendor bulk API (30% discount)
            results = await vendor_api.bulk_enrich(batch)
            
            # Publish all results
            for result in results:
                await message_bus.publish(
                    topic="agents:quality",
                    payload={"profile": result}
                )
```

**Savings:** Bulk API discount (30% off) = $0.014/profile instead of $0.02

---

### **Strategy 2: Smart Caching**

**Problem:** Re-enriching same profile multiple times wastes money

**Solution:** Local cache with TTL

```python
# In Data Enrichment Agent
from datetime import datetime, timedelta

profile_cache = {}  # In production: Redis with TTL

async def enrich_with_cache(url: str, ttl_days: int = 30):
    """Check cache before calling vendor API"""
    cache_key = f"profile:{url}"
    
    # Check cache
    if cache_key in profile_cache:
        cached = profile_cache[cache_key]
        if cached['expires_at'] > datetime.now():
            logger.info(f"Cache HIT: {url} (saved $0.02)")
            return cached['data']
    
    # Cache MISS: Call vendor API
    profile = await call_vendor_api(url)
    
    # Store in cache
    profile_cache[cache_key] = {
        'data': profile,
        'expires_at': datetime.now() + timedelta(days=ttl_days)
    }
    
    return profile
```

**Savings:** 50% cache hit rate = 50% cost reduction

---

### **Strategy 3: Tiered Vendor Selection**

**Problem:** Proxycurl ($0.04) more expensive than Nubela ($0.02)

**Solution:** Use cheaper vendor first, upgrade if needed

```python
async def smart_enrich(url: str, quality_required: str = "standard"):
    """Select vendor based on quality requirements"""
    
    if quality_required == "high":
        # Use Proxycurl (highest quality, $0.04)
        return await call_proxycurl(url)
    
    elif quality_required == "standard":
        # Use Nubela (good quality, $0.02)
        return await call_nubela(url)
    
    else:  # "basic"
        # Use Google CSE + manual parsing (free)
        return await parse_google_result(url)
```

**Savings:** Use Nubela by default (50% cheaper than Proxycurl)

---

### **Strategy 4: Predictive Pre-Fetching**

**Problem:** User waits 2-3 seconds for enrichment API call

**Solution:** Pre-fetch likely reveals based on quality score

```python
async def predictive_prefetch(light_profiles: List[Dict]):
    """Pre-enrich profiles likely to be revealed"""
    
    # Score profiles by likelihood of reveal
    # (based on title match, location, etc.)
    scores = []
    for profile in light_profiles:
        score = calculate_reveal_probability(profile)
        scores.append((profile, score))
    
    # Pre-fetch top 5 most likely
    sorted_profiles = sorted(scores, key=lambda x: x[1], reverse=True)
    top_5 = [p for p, s in sorted_profiles[:5] if s > 0.7]
    
    # Enrich in background (before user clicks)
    for profile in top_5:
        asyncio.create_task(enrich_with_cache(profile['url']))
```

**Benefit:** Instant reveal (0s latency) for top candidates

---

## ðŸš€ Implementation Roadmap

### **Week 1: Data Enrichment Agent**
- [ ] Create `/agents/data-enrichment-agent/` directory
- [ ] Implement Proxycurl API client
- [ ] Implement Nubela API client
- [ ] Implement Google CSE API client
- [ ] Add pay-to-reveal logic (credit system)
- [ ] Add audit logging (GDPR compliance)
- [ ] Write unit tests (>80% coverage)
- [ ] Add to `docker-compose.yml` (Port 8097)

### **Week 2: Proactive Scanning Enhancement**
- [ ] Add Google X-Ray search function
- [ ] Add discovery-only mode (no enrichment)
- [ ] Update message handlers for light profiles
- [ ] Add platform-specific search templates
- [ ] Test with 100 free Google CSE queries

### **Week 3: Scout Coordinator Updates**
- [ ] Add `/pipelines/start-discovery` endpoint
- [ ] Add `/pipelines/{id}/reveal` endpoint
- [ ] Add credit checking logic
- [ ] Update pipeline state machine (discovery â†’ enrichment)
- [ ] Add cost tracking per pipeline

### **Week 4: Caching & Optimization**
- [ ] Implement Redis cache for enriched profiles
- [ ] Add batch enrichment worker
- [ ] Add predictive pre-fetching
- [ ] Add tiered vendor selection
- [ ] Benchmark cost savings (target: 50% reduction)

### **Week 5: UI Integration**
- [ ] Desktop app: Light profile cards UI
- [ ] Desktop app: "Reveal" button with cost estimate
- [ ] Desktop app: Credit balance display
- [ ] Desktop app: Enrichment progress bar
- [ ] Desktop app: Cost analytics dashboard

### **Week 6: Testing & Launch**
- [ ] End-to-end integration tests
- [ ] Load testing (1,000 profiles/hour)
- [ ] Cost validation (compare to manual workflow)
- [ ] GDPR compliance audit
- [ ] Beta launch with 10 users

---

## ðŸ“Š Expected Performance Metrics

### **Before (Manual Workflow)**
- **Discovery:** 30 minutes (manual LinkedIn search)
- **Enrichment:** 1 hour (manual copy-paste)
- **Cost:** $0 (but 1.5 hours of recruiter time @ $50/hr = $75 value)
- **Quality:** Low (no AI screening)
- **Compliance:** Manual (high risk)

### **After (Automated with Agents + Vendor APIs)**
- **Discovery:** 2 minutes (Google X-Ray automation)
- **Enrichment:** 5 minutes (vendor API batch)
- **Cost:** $10 (500 profiles @ $0.02 each)
- **Quality:** High (AI scoring via Quality-Focused Agent)
- **Compliance:** Automatic (audit logging, GDPR tools)

### **Efficiency Gains**
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Time per 500 profiles** | 12.5 hours | 7 minutes | **107x faster** |
| **Cost per 500 profiles** | $625 (recruiter time) | $10 (API) | **98% cheaper** |
| **Quality score** | Manual (60%) | AI-powered (85%) | **+25% accuracy** |
| **Compliance risk** | High | Low | **97% compliant** |

---

## ðŸ”§ Technical Implementation Details

### **New Agent Structure**

```
agents/
â”œâ”€â”€ data-enrichment-agent/           # NEW (Port 8097)
â”‚   â”œâ”€â”€ main.py                      # FastAPI app
â”‚   â”œâ”€â”€ vendors/
â”‚   â”‚   â”œâ”€â”€ proxycurl.py             # Proxycurl API client
â”‚   â”‚   â”œâ”€â”€ nubela.py                # Nubela API client
â”‚   â”‚   â””â”€â”€ google_cse.py            # Google Custom Search
â”‚   â”œâ”€â”€ cache.py                     # Redis cache layer
â”‚   â”œâ”€â”€ credits.py                   # User credit system
â”‚   â”œâ”€â”€ audit.py                     # GDPR audit logging
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ proactive-scanning-agent/        # ENHANCED
â”‚   â”œâ”€â”€ main.py                      # Add Google X-Ray
â”‚   â”œâ”€â”€ scanners/
â”‚   â”‚   â”œâ”€â”€ linkedin.py              # Existing
â”‚   â”‚   â”œâ”€â”€ github.py                # Existing
â”‚   â”‚   â”œâ”€â”€ stackoverflow.py         # Existing
â”‚   â”‚   â””â”€â”€ google_xray.py           # NEW
â”‚   â””â”€â”€ Dockerfile
â”‚
â””â”€â”€ scout-coordinator-agent/         # ENHANCED
    â”œâ”€â”€ main.py                      # Add discovery/reveal endpoints
    â”œâ”€â”€ credits.py                   # NEW (credit checking)
    â””â”€â”€ Dockerfile
```

### **Environment Variables**

```bash
# .env file additions
# Data Enrichment Agent
PROXYCURL_API_KEY=your-key-here
NUBELA_API_KEY=your-key-here
GOOGLE_CSE_API_KEY=your-key-here
GOOGLE_SEARCH_ENGINE_ID=your-id-here

# Rate Limits
PROXYCURL_RATE_LIMIT=100  # requests/minute
NUBELA_RATE_LIMIT=200     # requests/minute
GOOGLE_CSE_RATE_LIMIT=100 # requests/day (free tier)

# Costs (for tracking)
PROXYCURL_COST_PER_PROFILE=0.04
NUBELA_COST_PER_PROFILE=0.02
```

### **Docker Compose Update**

```yaml
# docker-compose.yml additions
services:
  data-enrichment-agent:
    build: ./agents/data-enrichment-agent
    ports:
      - "8097:8097"
    environment:
      - PROXYCURL_API_KEY=${PROXYCURL_API_KEY}
      - NUBELA_API_KEY=${NUBELA_API_KEY}
      - GOOGLE_CSE_API_KEY=${GOOGLE_CSE_API_KEY}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis
    restart: unless-stopped
```

---

## ðŸ“ˆ ROI Calculation

### **Scenario: 100 hires/year**

**Manual Workflow:**
- Time: 12.5 hours Ã— 100 = 1,250 hours
- Cost: 1,250 hours Ã— $50/hr = $62,500
- Quality: 60% hire rate (need 167 candidates for 100 hires)

**Automated Workflow:**
- Time: 7 minutes Ã— 100 = 11.7 hours
- Cost: (500 profiles Ã— $0.02) Ã— 100 = $1,000
- Quality: 85% hire rate (need 118 candidates for 100 hires)

**Savings:**
- **Time saved:** 1,238 hours/year
- **Money saved:** $61,500/year
- **Efficiency:** 30% fewer candidates needed (49 fewer sourcing cycles)

**Payback Period:** 1 month (after development costs amortized)

---

## ðŸŽ¯ Success Metrics

**Track these KPIs:**
1. **Discovery speed:** Time from query â†’ light profiles (target: <2 minutes)
2. **Enrichment speed:** Time from reveal â†’ full profile (target: <5 seconds)
3. **Cost per profile:** Average vendor API cost (target: <$0.02)
4. **Cache hit rate:** % of profiles served from cache (target: >50%)
5. **Quality score:** AI assessment accuracy (target: >85%)
6. **Compliance score:** GDPR/CCPA adherence (target: >95%)
7. **User satisfaction:** NPS score from recruiters (target: >50)

---

## ðŸ“š Related Documentation

- [SOURCING_COMPLIANCE.md](SOURCING_COMPLIANCE.md) - Legal framework
- [SOURCING_WORKFLOW_DETAILED.md](SOURCING_WORKFLOW_DETAILED.md) - Current agent workflow
- [AGENTS_ACHIEVEMENTS.md](AGENTS_ACHIEVEMENTS.md) - Agent capabilities
- [PRICING.md](PRICING.md) - Business model and costs
- [LOCAL_AI_ARCHITECTURE.md](LOCAL_AI_ARCHITECTURE.md) - Desktop app architecture

---

**Next Steps:**
1. Review this strategy with team
2. Prioritize Week 1-2 (Data Enrichment Agent + Google X-Ray)
3. Allocate development resources
4. Start implementation

**Questions?** Open a GitHub issue or contact: dev@opentalent.ai

---

**Version:** 1.0  
**Status:** âœ… Ready for Implementation  
**Next Review:** January 10, 2026
