models:
  # Base Granite model for interview intelligence
  granite4:350m-h:
    architecture: granite
    size: 350m
    quantization: 4bit
    max_context: 2048
    fine_tuning:
      supported: true
      technique: lora
      rank: 16

  # Fine-tuned Granite model for interviews
  granite-interview-ft:
    architecture: granite
    size: 350m
    quantization: 4bit
    max_context: 2048
    base_model: granite4:350m-h
    fine_tuning:
      supported: false  # Already fine-tuned
      technique: lora
      rank: 16
      dataset: interview_v1

  # Llama 2 models for comparison and A/B testing
  llama-2-7b-chat:
    architecture: llama
    size: 7b
    quantization: 4bit
    max_context: 4096
    fine_tuning:
      supported: true
      technique: qlora
      rank: 64

  llama-2-13b-chat:
    architecture: llama
    size: 13b
    quantization: 4bit
    max_context: 4096
    fine_tuning:
      supported: true
      technique: qlora
      rank: 64

  # Mistral models for advanced reasoning
  mistral-7b-instruct:
    architecture: mistral
    size: 7b
    quantization: 4bit
    max_context: 4096
    fine_tuning:
      supported: true
      technique: qlora
      rank: 64

  mistral-7b-instruct-v0.2:
    architecture: mistral
    size: 7b
    quantization: 4bit
    max_context: 4096
    fine_tuning:
      supported: true
      technique: qlora
      rank: 64

  # Larger models for enterprise use (requires significant hardware)
  llama-2-70b-chat:
    architecture: llama
    size: 70b
    quantization: 4bit
    max_context: 4096
    fine_tuning:
      supported: true
      technique: qlora
      rank: 64

  # Custom fine-tuned models
  granite-interview-advanced:
    architecture: granite
    size: 350m
    quantization: 4bit
    max_context: 2048
    base_model: granite4:350m-h
    fine_tuning:
      supported: false
      technique: lora
      rank: 32  # Higher rank for more complex fine-tuning
      dataset: interview_advanced_v1

  llama-interview-ft:
    architecture: llama
    size: 7b
    quantization: 4bit
    max_context: 4096
    base_model: llama-2-7b-chat
    fine_tuning:
      supported: false
      technique: qlora
      rank: 64
      dataset: interview_v1