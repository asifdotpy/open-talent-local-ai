# TalentAI Conversation Service Environment Variables
# Copy this file to .env and fill in the values

# Service Configuration
SERVICE_NAME=conversation-service
SERVICE_PORT=8003
HOST=0.0.0.0

# LLM Provider Configuration
# Options: ollama, openai, peft, vllm, mock
LLM_PROVIDER=ollama
LLM_MODEL=technical-interviewer
LLM_API_KEY=
LLM_BASE_URL=http://localhost:11434
LLM_TIMEOUT=300
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048

# LoRA Adapter Configuration (for vLLM with personas)
LLM_LORA_ADAPTER=technical  # Options: technical, behavioral, hr

# Fallback LLM Provider (optional)
LLM_FALLBACK_PROVIDER=ollama
LLM_FALLBACK_MODEL=granite4:350m-h
LLM_FALLBACK_API_KEY=
LLM_FALLBACK_BASE_URL=http://localhost:11434
LLM_FALLBACK_TIMEOUT=300
LLM_FALLBACK_TEMPERATURE=0.7
LLM_FALLBACK_MAX_TOKENS=2048

# Legacy Ollama Configuration (for backward compatibility)
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=granite4:350m-h
OLLAMA_TIMEOUT=300

# PEFT Configuration (for direct transformers usage with 450M model)
PEFT_MODEL=asifdotpy/vetta-granite-450m-lora-v1
PEFT_BASE_MODEL=ibm-granite/granite-4.0-450m-instruct

# API Configuration
API_PREFIX=/api/v1
DEBUG=false

# Logging
LOG_LEVEL=INFO
LOG_FILE=/app/logs/conversation_service.log

# Performance Tuning
MAX_WORKERS=4
REQUEST_TIMEOUT=60
MAX_RETRIES=3

# Health Check
HEALTH_CHECK_INTERVAL=30
HEALTH_CHECK_TIMEOUT=10

# Question Generation Settings
DEFAULT_NUM_QUESTIONS=10
MAX_NUM_QUESTIONS=20
DEFAULT_DIFFICULTY=medium
SUPPORTED_DIFFICULTIES=easy,medium,hard