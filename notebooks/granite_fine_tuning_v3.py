# -*- coding: utf-8 -*-
"""granite_fine_tuning_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r54VOdCxtwy5WHK8mm-0awJyxO-6TxEf

### Mount Google Drive for training files
"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  Mount Google Drive                                                       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from google.colab import drive
import os

# Mount Google Drive
print("ğŸ”— Mounting Google Drive...")
drive.mount('/content/drive')

# Create project directory structure on Drive
project_dir = '/content/drive/MyDrive/talent-ai-vetta'
checkpoints_dir = f'{project_dir}/checkpoints'
models_dir = f'{project_dir}/models'

# Create directories if they don't exist
os.makedirs(checkpoints_dir, exist_ok=True)
os.makedirs(models_dir, exist_ok=True)
os.makedirs(f'{models_dir}/lora', exist_ok=True)
os.makedirs(f'{models_dir}/merged', exist_ok=True)
os.makedirs(f'{models_dir}/gguf', exist_ok=True)

print("âœ… Google Drive mounted successfully!")
print(f"ğŸ“ Project directory: {project_dir}")
print(f"ğŸ“ Checkpoints: {checkpoints_dir}")
print(f"ğŸ“ Models: {models_dir}")

# Verify Drive is accessible
try:
    test_file = f'{project_dir}/test_write.txt'
    with open(test_file, 'w') as f:
        f.write('Drive is accessible!')
    os.remove(test_file)
    print("âœ… Drive write test passed - persistent storage ready!")
except Exception as e:
    print(f"âš ï¸  Drive write test failed: {e}")
    print("   You may need to remount Drive or check permissions")

"""### Check Status"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  Check Drive Status Script                                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
WHEN TO RUN: After mounting Google Drive (CELL 1) and before training/saving.

PURPOSE: Check if models are already saved on Google Drive to avoid re-saving.
This prevents wasting time retraining or re-saving models that already exist.

USAGE IN NOTEBOOK:
1. Mount Drive: Run CELL 1
2. Check status: !python check_drive_status.py
3. If models exist: Skip to upload (CELL 8)
4. If no models: Continue with training (CELL 5) then saving (CELL 7)
"""

import os

def check_drive_status():
    project_dir = '/content/drive/MyDrive/talent-ai-vetta'
    checkpoints_dir = f'{project_dir}/checkpoints'
    models_dir = f'{project_dir}/models'

    print("ğŸ” Checking Google Drive status...")
    print(f"ğŸ“ Project: {project_dir}")
    print("   This shows what's already saved to avoid retraining!\n")

    # Check checkpoints
    if os.path.exists(checkpoints_dir):
        checkpoints = [d for d in os.listdir(checkpoints_dir) if d.startswith('checkpoint-')]
        if checkpoints:
            latest = max(checkpoints, key=lambda x: int(x.split('-')[1]))
            print(f"âœ… Checkpoints: {len(checkpoints)} found, latest: {latest}")
            print("   ğŸ’¡ You can RESUME training from here if needed!")
        else:
            print("ğŸ“‚ Checkpoints directory exists but no checkpoints found")
    else:
        print("âŒ No checkpoints directory found")

    # Check saved models
    models_found = []
    for model_type in ['lora', 'merged', 'gguf']:
        model_path = f'{models_dir}/{model_type}'
        if os.path.exists(model_path) and os.listdir(model_path):
            # Calculate size
            size = sum(os.path.getsize(os.path.join(model_path, f))
                      for f in os.listdir(model_path)
                      if os.path.isfile(os.path.join(model_path, f)))
            size_gb = size / (1024**3)
            print(f"âœ… {model_type.upper()}: {size_gb:.2f} GB saved")
            models_found.append(model_type)
        else:
            print(f"âŒ {model_type.upper()}: Not found")

    print("\n" + "="*60)
    if models_found:
        print("ğŸ‰ MODELS ALREADY SAVED! You can skip training/saving!")
        print(f"Saved models: {', '.join(models_found).upper()}")
        print("\nğŸš€ NEXT: Run upload cell (CELL 8) to publish to Hugging Face")
        print("   No need to train or save again!")
    else:
        print("ğŸ“ No saved models found.")
        print("\nğŸš€ NEXT: Run training (CELL 5) then save models (CELL 7)")
        print("   Models will be saved to Drive automatically")

    if os.path.exists(checkpoints_dir) and checkpoints:
        print("ğŸ’¾ Training checkpoints available - can resume training if interrupted.")

    print("\n" + "="*60)
    print("ğŸ’¡ DRIVE PERSISTENCE: Even if Colab crashes, your progress is safe!")
    print("   Run this script anytime to check status.")

if __name__ == "__main__":
    check_drive_status()

"""### Copy Data"""

import shutil
import os
import time # Import time for delay

source_data_path = '/content/drive/MyDrive/Open Talent/data'
destination_colab_path = '.' # Copy to the current working directory

# First check for existence
if not os.path.exists(source_data_path):
    print(f"âŒ Source directory '{source_data_path}' not found on first check.")
    print("It's possible Google Drive momentarily disconnected. Retrying check after a short delay...")
    time.sleep(5) # Wait for 5 seconds

if os.path.exists(source_data_path):
    print(f"Copying contents from '{source_data_path}' to '{destination_colab_path}'...")

    # Iterate through items and copy them
    for item_name in os.listdir(source_data_path):
        source_item = os.path.join(source_data_path, item_name)
        destination_item = os.path.join(destination_colab_path, item_name)

        if os.path.isfile(source_item):
            shutil.copy2(source_item, destination_item)
        elif os.path.isdir(source_item):
            shutil.copytree(source_item, destination_item, dirs_exist_ok=True)

    print("âœ… Contents copied successfully!")
    # Verify contents in the current directory
    print(f"Contents of current directory (after copy):")
    for item in os.listdir(destination_colab_path):
        if item.startswith('vetta_') or item == 'enhanced_vetta_data.py' or item == 'README.md': # Only show relevant copied files
            print(f"- {item}")
else:
    print(f"âŒ Source directory '{source_data_path}' still not found after retry.")
    print("This indicates a persistent issue. Please ensure your Google Drive is correctly mounted and consider restarting the Colab runtime.")

# Commented out IPython magic to ensure Python compatibility.
# # â•—â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# # â•‘  CELL 1: Install Dependencies                                             â•‘
# # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 
# %%capture
# print("ğŸš€ Starting dependency installation...")
# 
# # Conditional package installation - only install if not already correct
# import subprocess
# import sys
# from packaging import version
# 
# def check_package_version(package, version_spec):
#     """Check if installed package version satisfies the version spec."""
#     try:
#         result = subprocess.run([sys.executable, '-c', f'import {package}; print({package}.__version__)'],
#                               capture_output=True, text=True)
#         if result.returncode == 0:
#             installed_version = result.stdout.strip()
#             # Parse version spec (e.g., '<0.9.0')
#             if version_spec.startswith('<'):
#                 max_version = version_spec[1:]
#                 return version.parse(installed_version) < version.parse(max_version)
#             elif version_spec.startswith('>='):
#                 min_version = version_spec[2:]
#                 return version.parse(installed_version) >= version.parse(min_version)
#             else:
#                 return installed_version == version_spec
#     except Exception as e:
#         print(f"Version check failed: {e}")
#         pass
#     return False
# 
# # Check if key packages are already installed correctly
# packages_ok = (
#     check_package_version('trl', '<0.9.0') and
#     check_package_version('unsloth', '>=2025.11.4')  # Check for recent unsloth version
# )
# 
# if not packages_ok:
#     print("ğŸ“¦ Installing required packages...")
#     # Install trl first to avoid version conflicts
#     !pip install --no-deps "trl<0.9.0"
#     # Then install the rest
#     !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
#     !pip install --no-deps "xformers<0.0.27" peft accelerate bitsandbytes
#     print("âœ… Dependencies installed - please restart runtime if prompted")
# else:
#     print("âœ… Dependencies already installed correctly - skipping installation")
# 
# print("âœ… Dependencies ready")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 2: Configuration                                                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class Config:
    """Training configuration."""
    # Model
    model_name: str = "ibm-granite/granite-3.0-2b-instruct"
    max_seq_length: int = 2048
    load_in_4bit: bool = True

    # LoRA
    lora_r: int = 16
    lora_alpha: int = 16
    lora_dropout: float = 0
    target_modules: tuple = ("q_proj", "k_proj", "v_proj", "o_proj",
                             "gate_proj", "up_proj", "down_proj")

    # Training - REDUCED for better dataset
    batch_size: int = 4  # Updated from 2 to 4
    gradient_accumulation: int = 2 # Updated from 4 to 2
    epochs: int = 5  # Changed from 1 to 5
    learning_rate: float = 2e-4
    warmup_steps: int = 10

    # Output - Use Google Drive for persistence
    output_dir: str = "/content/drive/MyDrive/talent-ai-vetta/checkpoints"
    models_dir: str = "/content/drive/MyDrive/talent-ai-vetta/models"

    # Data
    data_repetitions: int = 10  # Updated to 10

config = Config()
print(f"ğŸ¯ Model: {config.model_name}")
print(f"ğŸ“¦ Output: {config.output_dir}")
print(f"âš¡ Batch: {config.batch_size} Ã— {config.gradient_accumulation} = {config.batch_size * config.gradient_accumulation} effective")
print(f"ğŸ“š Epochs: {config.epochs} (optimized for {config.data_repetitions}Ã— dataset)")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 3: Load Model with Unsloth                                          â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from unsloth import FastLanguageModel
import torch

print(f"ğŸ”ï¸ Loading {config.model_name}...")

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=config.model_name,
    max_seq_length=config.max_seq_length,
    dtype=None,  # Auto-detect: Float16 for T4, BFloat16 for Ampere+
    load_in_4bit=config.load_in_4bit,
)

print(f"âœ… Model loaded | GPU: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=config.lora_r,
    target_modules=list(config.target_modules),
    lora_alpha=config.lora_alpha,
    lora_dropout=config.lora_dropout,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

print(f"âœ… LoRA adapters added")
model.print_trainable_parameters()

"""### Updated dataset generation"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  ENHANCED VETTA DATASET GENERATOR - Copy this entire cell to Colab      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Enhanced Vetta Training Data Generator
=====================================

This module provides an improved dataset for fine-tuning Vetta, the AI interviewer.
Based on evaluation feedback, this version uses Hugging Face datasets for efficiency.

- Loads pre-existing datasets from user's Hugging Face account (asifdotpy)
- Falls back to databricks-dolly-15k if no user datasets found
- Filters and adapts data for interview scenarios
- Includes metadata and validation
- Train/validation split capability

Usage:
    python enhanced_vetta_data.py  # Generates and validates dataset
"""

from collections import Counter, defaultdict
from typing import List, Dict, Any, Tuple
import json
import random
from dataclasses import dataclass
from datasets import load_dataset, DatasetDict
from huggingface_hub import HfApi
import os


@dataclass
class ExampleMetadata:
    """Metadata for training examples."""
    category: str  # opening, technical_question, feedback, behavioral, closing, edge_case, multi_turn
    difficulty: str  # beginner, intermediate, advanced
    domain: str  # backend_python, frontend, ml_ai, system_design, behavioral, general
    expected_length: str  # short, medium, long
    has_context: bool = False  # Whether this is part of a multi-turn conversation


def validate_dataset(examples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Validate dataset quality and provide statistics.

    Returns comprehensive validation report.
    """
    validation_report = {
        "total_examples": len(examples),
        "warnings": [],
        "errors": [],
        "statistics": {}
    }

    # Category distribution
    categories = Counter(ex.get('category', 'unknown') for ex in examples)
    validation_report["statistics"]["categories"] = dict(categories)

    # Check category balance
    total = len(examples)
    max_category_pct = max(categories.values()) / total
    if max_category_pct > 0.4:
        validation_report["warnings"].append(
            f"Imbalanced categories - largest category represents {max_category_pct:.1%} of dataset"
        )

    # Response length analysis
    response_lengths = []
    for ex in examples:
        response = ex.get('response', '')
        word_count = len(response.split())
        response_lengths.append(word_count)

    if response_lengths:
        min_len, max_len = min(response_lengths), max(response_lengths)
        avg_len = sum(response_lengths) / len(response_lengths)
        validation_report["statistics"]["response_lengths"] = {
            "min": min_len,
            "max": max_len,
            "avg": round(avg_len, 1)
        }

        # Check for length variety
        if max_len - min_len < 50:
            validation_report["warnings"].append(
                "Limited response length variety - add more short and long responses"
            )

    # Dataset size check
    if total < 200:
        validation_report["warnings"].append(
            "Dataset too small for fine-tuning - aim for 500+ examples"
        )
    elif total < 500:
        validation_report["warnings"].append(
            "Dataset size adequate for basic training but consider expansion for better generalization"
        )

    # Difficulty distribution
    difficulties = Counter(ex.get('difficulty', 'unknown') for ex in examples)
    validation_report["statistics"]["difficulties"] = dict(difficulties)

    # Domain distribution
    domains = Counter(ex.get('domain', 'unknown') for ex in examples)
    validation_report["statistics"]["domains"] = dict(domains)

    # Multi-turn examples
    multi_turn_count = sum(1 for ex in examples if ex.get('has_context', False))
    validation_report["statistics"]["multi_turn_examples"] = multi_turn_count

    if multi_turn_count < total * 0.1:
        validation_report["warnings"].append(
            "Few multi-turn conversation examples - add more context-aware responses"
        )

    return validation_report


def create_enhanced_vetta_examples() -> List[Dict[str, Any]]:
    """
    Create an enhanced dataset using Hugging Face datasets.
    Loads pre-existing data from user's Hugging Face account and adapts it for Vetta interview training.
    """

    # Set Hugging Face token for authentication
    os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-huggingface-token-here')

    # Load the specific dataset from user's Hugging Face account
    dataset_name = "asifdotpy/vetta-interview-dataset"
    print(f"ğŸ“¥ Loading dataset from your Hugging Face account: {dataset_name}")

    dataset = load_dataset(dataset_name, split="train")

    examples = []
    interview_categories = {
        "opening": "opening",
        "technical_question": "technical_question",
        "closing": "closing",
        "sourcing_initiation": "general",
        "behavioral": "behavioral",
        "agent_coordination": "general",
        "feedback": "feedback",
        "multi_turn": "multi_turn",
        "persona_adaptation": "general",
        "workflow_orchestration": "general",
        "platform_navigation": "general",
        "candidate_profiling": "general",
        "execution_monitoring": "general",
        "data_driven_insights": "general",
        "edge_case": "edge_case"
    }

    # Process and categorize examples
    for item in dataset:
        instruction = item.get('instruction', '')
        response = item.get('response', '')
        category = item.get('category', '')

        # Map to interview categories
        vetta_category = interview_categories.get(category, "general")

        # Determine difficulty and domain based on content
        difficulty = "intermediate"
        domain = "general"
        if "python" in instruction.lower() or "code" in instruction.lower():
            domain = "backend_python"
            difficulty = "advanced" if "advanced" in instruction.lower() else "intermediate"
        elif "javascript" in instruction.lower() or "react" in instruction.lower():
            domain = "frontend"
        elif "machine learning" in instruction.lower() or "ml" in instruction.lower():
            domain = "ml_ai"
        elif "system design" in instruction.lower():
            domain = "system_design"

        # Determine expected length
        word_count = len(response.split())
        expected_length = "short" if word_count < 50 else "medium" if word_count < 150 else "long"

        # Check for multi-turn context
        has_context = "follow up" in instruction.lower() or "previous" in instruction.lower()

        examples.append({
            "instruction": instruction,
            "response": response,
            "category": vetta_category,
            "difficulty": difficulty,
            "domain": domain,
            "expected_length": expected_length,
            "has_context": has_context
        })

        # Limit to ~500 examples for efficiency
        if len(examples) >= 500:
            break

    return examples


def create_train_validation_split(examples: List[Dict[str, Any]],
                                train_ratio: float = 0.8,
                                stratify_by: str = "category") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    Create stratified train/validation split.

    Args:
        examples: List of training examples
        train_ratio: Ratio of examples for training (0.0-1.0)
        stratify_by: Field to stratify by (e.g., 'category', 'difficulty')

    Returns:
        Tuple of (train_examples, val_examples)
    """
    # Group by stratification field
    groups = defaultdict(list)
    for ex in examples:
        key = ex.get(stratify_by, 'unknown')
        groups[key].append(ex)

    train_examples = []
    val_examples = []

    for group_examples in groups.values():
        random.shuffle(group_examples)
        split_idx = int(len(group_examples) * train_ratio)
        train_examples.extend(group_examples[:split_idx])
        val_examples.extend(group_examples[split_idx:])

    # Shuffle final datasets
    random.shuffle(train_examples)
    random.shuffle(val_examples)

    return train_examples, val_examples


def save_dataset(examples: List[Dict[str, Any]], filename: str):
    """Save dataset to JSON file."""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(examples, f, indent=2, ensure_ascii=False)
    print(f"âœ… Saved {len(examples)} examples to {filename}")


def main():
    """Generate and validate the enhanced dataset."""
    print("ğŸš€ Generating Enhanced Vetta Training Dataset...")
    print("=" * 60)

    # Generate examples
    examples = create_enhanced_vetta_examples()
    print(f"ğŸ“Š Generated {len(examples)} examples")

    # Validate dataset
    print("\nğŸ” Validating dataset...")
    validation = validate_dataset(examples)

    print(f"Total examples: {validation['total_examples']}")
    print(f"Categories: {validation['statistics']['categories']}")
    print(f"Response lengths: {validation['statistics']['response_lengths']}")
    print(f"Multi-turn examples: {validation['statistics']['multi_turn_examples']}")

    if validation['warnings']:
        print("\nâš ï¸ Warnings:")
        for warning in validation['warnings']:
            print(f"  - {warning}")

    if validation['errors']:
        print("\nâŒ Errors:")
        for error in validation['errors']:
            print(f"  - {error}")

    # Create train/validation split
    print("\nâœ‚ï¸ Creating train/validation split...")
    train_examples, val_examples = create_train_validation_split(examples)
    print(f"Train: {len(train_examples)} examples")
    print(f"Validation: {len(val_examples)} examples")

    # Save datasets
    print("\nğŸ’¾ Saving datasets...")
    save_dataset(train_examples, "vetta_train.json")
    save_dataset(val_examples, "vetta_validation.json")
    save_dataset(examples, "vetta_full.json")

    print("\nâœ… Dataset generation complete!")
    print("ğŸ“ Files created:")
    print("  - vetta_full.json (complete dataset)")
    print("  - vetta_train.json (training split)")
    print("  - vetta_validation.json (validation split)")


if __name__ == "__main__":
    main()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 4: Prepare Vetta Training Data                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from datasets import Dataset
import json

# Load configuration (for standalone execution)
try:
    # Try to use config from previous cell (Colab style)
    config
    print("âš™ï¸ Using config from previous cell...")
except NameError:
    # Fallback for standalone execution
    print("âš™ï¸ Loading configuration for standalone execution...")
    exec(open('04_config.py').read())

# Load enhanced training data from JSON file
print("ğŸ“š Loading enhanced Vetta training dataset...")
try:
    with open('vetta_train.json', 'r', encoding='utf-8') as f:
        train_examples = json.load(f)
    print(f"âœ… Loaded {len(train_examples)} training examples")
except FileNotFoundError:
    print("âŒ vetta_train.json not found. Run the dataset generator cell first")
    print("ğŸ”„ Falling back to original dataset...")
    # Fallback to original examples if enhanced data not available
    train_examples = [
        {
            "instruction": "Begin a software engineering interview for a senior Python developer position.",
            "response": "Hello! I'm Vetta, and I'll be conducting your interview today for the Senior Python Developer role. I'm excited to learn about your experience! Let's start with something to help me understand your background. Could you walk me through a challenging Python project you've worked on recently? I'm particularly interested in the architectural decisions you made and any trade-offs you considered."
        },
        {
            "instruction": "The candidate says they have 5 years of Python experience but seems nervous.",
            "response": "That's wonderful - five years gives you a solid foundation! I can tell you have valuable experience to share. Take your time, there's no rush here. Let's make this conversational rather than interrogative. What aspect of Python development do you find most engaging? Is it backend systems, data processing, automation, or something else entirely?"
        },
        {
            "instruction": "Ask a technical question about Python async programming.",
            "response": "Let's explore async programming. Can you explain the difference between `asyncio.gather()` and `asyncio.wait()`, and describe a scenario where you'd choose one over the other? Feel free to think through it - I'm interested in your reasoning process as much as the answer."
        },
        {
            "instruction": "The candidate gave a partially correct answer about async. Provide feedback.",
            "response": "You've got the core concept right - both do handle multiple coroutines concurrently. Let me add some nuance: `gather()` is great when you want all results together and any exception to propagate immediately. `wait()` gives you more control - you can handle completed and pending tasks separately, which is useful for timeout scenarios. Your instinct about concurrency patterns is solid though!"
        },
        {
            "instruction": "Wrap up the interview professionally.",
            "response": "Thank you so much for your time today! You've demonstrated strong technical knowledge, especially in [specific area]. I appreciated your thoughtful approach to problem-solving. Do you have any questions for me about the role or the team? After this, our hiring team will review and you'll hear back within [timeframe]. Best of luck, and it was genuinely a pleasure speaking with you!"
        },
        {
            "instruction": "Ask about system design for a microservices architecture.",
            "response": "Let's discuss system design. Imagine you're designing a new microservices-based e-commerce platform. How would you handle inter-service communication? What patterns would you use for data consistency across services? Walk me through your thinking - I'd love to understand your approach to distributed systems."
        },
        {
            "instruction": "The candidate struggles with a coding question. Help them.",
            "response": "I can see you're working through this - that's exactly the kind of problem-solving we value. Let me give you a hint: think about what data structure would give you O(1) lookup time. Sometimes stepping back and considering the problem from a different angle helps. What's the core operation you need to optimize here?"
        },
        {
            "instruction": "Evaluate a candidate's answer about database optimization.",
            "response": "Excellent points! You correctly identified indexing as crucial for query performance. I especially liked your mention of query execution plans - that shows you understand how to diagnose slow queries rather than just applying fixes blindly. One addition: have you worked with partitioning for very large tables? That's often the next step when indexes alone aren't enough."
        },
    ]

# Extract instruction/response pairs for training
VETTA_EXAMPLES = []
for ex in train_examples:
    VETTA_EXAMPLES.append({
        "instruction": ex["instruction"],
        "response": ex["response"]
    })

def format_example(ex: dict) -> dict:
    """Format example for Alpaca-style training."""
    return {
        "text": f"### Instruction:\n{ex['instruction']}\n\n### Response:\n{ex['response']}"
    }

# Create dataset with repetitions for reinforcement learning
training_data = []
for _ in range(config.data_repetitions):
    for example in VETTA_EXAMPLES:
        training_data.append(format_example(example))

dataset = Dataset.from_list(training_data)

print(f"âœ… Dataset: {len(dataset)} examples ({len(VETTA_EXAMPLES)} unique Ã— {config.data_repetitions} reps)")
print(f"\nğŸ“ Sample:\n{dataset[0]['text'][:400]}...")
print(f"\nğŸ“Š Dataset composition:")
categories = {}
for ex in train_examples:
    cat = ex.get('category', 'unknown')
    categories[cat] = categories.get(cat, 0) + 1
for cat, count in sorted(categories.items()):
    print(f"   {cat}: {count} examples")

print(f"Current Epochs: {config.epochs}")
print(f"Current Learning Rate: {config.learning_rate}")
print(f"Current Data Repetitions: {config.data_repetitions}")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 5: Train Vetta                                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# --- Re-load model and tokenizer if not in scope ---
# This part is crucial to ensure 'model' and 'tokenizer' are defined.
# Assumes 'config' and 'dataset' are already defined from previous cells.

# Note: config is assumed to be available from a previous cell. If not, it needs to be defined here.
# For safety, replicating the config definition temporarily if not guaranteed:

from transformers import TrainingArguments
from trl import SFTTrainer
import torch  # Also needed for torch.cuda.is_bf16_supported()


from dataclasses import dataclass
@dataclass
class Config:
    model_name: str = "ibm-granite/granite-3.0-2b-instruct"
    max_seq_length: int = 2048
    load_in_4bit: bool = True
    lora_r: int = 16; lora_alpha: int = 16; lora_dropout: float = 0
    target_modules: tuple = ("q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj")
    batch_size: int = 4 # Updated
    gradient_accumulation: int = 2 # Updated
    epochs: int = 5 # Updated
    learning_rate: float = 2e-4; warmup_steps: int = 10
    output_dir: str = "/content/drive/MyDrive/talent-ai-vetta/checkpoints"  # Drive path
    models_dir: str = "/content/drive/MyDrive/talent-ai-vetta/models"      # Drive path
    data_repetitions: int = 10 # Updated
config = Config()

# Check if model and tokenizer are defined. If not, load them.
if 'model' not in globals() or 'tokenizer' not in globals():
    print(f"ğŸ”„ Model or tokenizer not found in memory. Attempting to load for training...")
    # Load the base model
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=config.model_name,
        max_seq_length=config.max_seq_length,
        dtype=None,
        load_in_4bit=config.load_in_4bit,
    )
    # Add LoRA adapters
    model = FastLanguageModel.get_peft_model(
        model,
        r=config.lora_r,
        target_modules=list(config.target_modules),
        lora_alpha=config.lora_alpha,
        lora_dropout=config.lora_dropout,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=3407,
    )
    print("âœ… Model and tokenizer loaded successfully for training.")


training_args = TrainingArguments(
    # Batch settings
    per_device_train_batch_size=config.batch_size,
    gradient_accumulation_steps=config.gradient_accumulation,

    # Duration
    num_train_epochs=config.epochs,

    # Learning rate
    learning_rate=config.learning_rate,
    warmup_steps=config.warmup_steps,

    # Precision - auto-detect best for GPU
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),

    # Optimizer
    optim="adamw_8bit",
    weight_decay=0.01,

    # Logging & Checkpoints - Save more frequently to Drive for safety
    logging_steps=50,
    save_steps=100,   # Save every 100 steps to Drive (safer than 500)
    save_total_limit=5,  # Keep more checkpoints
    output_dir=config.output_dir,

    # Reproducibility
    seed=3407,
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=config.max_seq_length,
    args=training_args,
    packing=True, # Updated from False to True
)

print("ğŸš€ Starting Vetta training...")
print(f"   Epochs: {config.epochs}")
print(f"   Effective batch: {config.batch_size * config.gradient_accumulation}")
print(f"   Precision: {'BF16' if training_args.bf16 else 'FP16'}")
print(f"\nâ±ï¸  Expected time: ~45-60 minutes on T4 GPU\n")

stats = trainer.train()

print(f"\nâœ… Training complete!")
print(f"ğŸ“Š Final loss: {stats.training_loss:.4f}")
print(f"â±ï¸  Time: {stats.metrics['train_runtime'] / 60:.1f} minutes")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 6: Test Vetta                                                       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from unsloth import FastLanguageModel

# Enable fast inference
FastLanguageModel.for_inference(model)

TEST_PROMPTS = [
    "Begin a technical interview for a machine learning engineer position.",
    "The candidate seems unsure about their answer on neural networks. Encourage them.",
    "Ask a follow-up question about the candidate's experience with PyTorch.",
]

print("ğŸ§ª Testing Vetta's responses:\n")

for i, prompt in enumerate(TEST_PROMPTS, 1):
    inputs = tokenizer(
        f"### Instruction:\n{prompt}\n\n### Response:\n",
        return_tensors="pt"
    ).to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        do_sample=True,
        top_p=0.9,
        repetition_penalty=1.1,
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.split("### Response:")[-1].strip()

    print(f"ğŸ“ Test {i}: {prompt}")
    print(f"ğŸ¤– Vetta: {response[:350]}...\n")
    print("-" * 60)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 7: Save Model                                                       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
from unsloth import FastLanguageModel

# Load configuration (for standalone execution)
try:
    # Try to use config from previous cell (Colab style)
    config
    print("âš™ï¸ Using config from previous cell...")
except NameError:
    # Fallback for standalone execution
    print("âš™ï¸ Loading configuration for standalone execution...")
    exec(open('04_config.py').read())

# Check if model and tokenizer are defined. If not, try to load from checkpoint or base model.
if 'model' not in globals() or 'tokenizer' not in globals():
    print("ğŸ”„ Model or tokenizer not found in memory. Attempting to load...")

    # First, try to load from the latest checkpoint if available
    checkpoint_dir = config.output_dir
    if os.path.exists(checkpoint_dir):
        # Find the latest checkpoint
        checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]
        if checkpoints:
            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))
            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)
            print(f"ğŸ“‚ Found checkpoint: {checkpoint_path}")
            try:
                model, tokenizer = FastLanguageModel.from_pretrained(
                    model_name=checkpoint_path,
                    max_seq_length=config.max_seq_length,
                    dtype=None,
                    load_in_4bit=config.load_in_4bit,
                )
                print("âœ… Loaded model from latest checkpoint.")
            except Exception as e:
                print(f"âŒ Failed to load from checkpoint: {e}")
                print("ğŸ”„ Falling back to loading base model...")
                # Load the base model
                model, tokenizer = FastLanguageModel.from_pretrained(
                    model_name=config.model_name,
                    max_seq_length=config.max_seq_length,
                    dtype=None,
                    load_in_4bit=config.load_in_4bit,
                )
                print("âœ… Loaded base model.")
        else:
            print("ğŸ“‚ No checkpoints found, loading base model...")
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=config.model_name,
                max_seq_length=config.max_seq_length,
                dtype=None,
                load_in_4bit=config.load_in_4bit,
            )
            print("âœ… Loaded base model.")
    else:
        print("ğŸ“‚ No checkpoint directory found, loading base model...")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=config.model_name,
            max_seq_length=config.max_seq_length,
            dtype=None,
            load_in_4bit=config.load_in_4bit,
        )
        print("âœ… Loaded base model.")

    # Add LoRA adapters (assuming we need to add them if loading base model)
    model = FastLanguageModel.get_peft_model(
        model,
        r=config.lora_r,
        target_modules=list(config.target_modules),
        lora_alpha=config.lora_alpha,
        lora_dropout=config.lora_dropout,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=3407,
    )
    print("âœ… LoRA adapters added.")

# Directories - Use Drive for persistence
lora_dir = f"{config.models_dir}/lora"
merged_dir = f"{config.models_dir}/merged"
gguf_dir = f"{config.models_dir}/gguf"

# Ensure directories exist
os.makedirs(config.models_dir, exist_ok=True)
os.makedirs(lora_dir, exist_ok=True)
os.makedirs(merged_dir, exist_ok=True)
os.makedirs(gguf_dir, exist_ok=True)

# 1. Save LoRA adapters (~50MB)
print("ğŸ’¾ Saving LoRA adapters...")
model.save_pretrained(lora_dir)
tokenizer.save_pretrained(lora_dir)
print(f"   âœ… {lora_dir}")

# 2. Save merged model (~4GB) - SKIPPED in Colab to prevent session resets
print("\nğŸ’¾ Skipping merged model save in Colab (memory intensive)...")
print("   â„¹ï¸  Merge locally using: python merge_lora.py")
print("   âœ… Skipped to prevent Colab crashes")

# 3. Save GGUF for Ollama (~1.5GB)
print("\nğŸ’¾ Skipping GGUF save in Colab (memory intensive)...")
print("   â„¹ï¸  GGUF conversion requires ~16GB+ RAM and compilation")
print("   â„¹ï¸  Convert locally using: python convert_to_gguf.py")
print("   âœ… Skipped to prevent Colab crashes")

# Show sizes
print("\nğŸ“Š Model sizes:")
for name, path in [("LoRA", lora_dir), ("Merged", merged_dir), ("GGUF", gguf_dir)]:
    if os.path.exists(path):
        size = sum(os.path.getsize(os.path.join(path, f))
                   for f in os.listdir(path)
                   if os.path.isfile(os.path.join(path, f)))
        if size > 0:
            print(f"   {name}: {size / 1024**2:.1f} MB")
        else:
            print(f"   {name}: Skipped (directory empty)")
    else:
        print(f"   {name}: Directory not created")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 8: Upload to Hugging Face Hub                                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
from huggingface_hub import login, HfApi
from google.colab import userdata
from unsloth import FastLanguageModel

# 1. Retrieve HuggingFace token securely
try:
    token = userdata.get('HF_TOKEN')
    login(token=token)
    api = HfApi(token=token)
except Exception:
    print("âš ï¸ Error: Could not log in. Make sure 'HF_TOKEN' is set in Colab Secrets.")
    raise

# 2. CONFIGURATION
HF_USERNAME = "asifdotpy"
BASE_MODEL_NAME = "vetta-granite-2b"
VERSION = "v3"  # Updated version for new training
REPO_ID_MERGED = f"{HF_USERNAME}/{BASE_MODEL_NAME}-{VERSION}"
REPO_ID_LORA = f"{HF_USERNAME}/{BASE_MODEL_NAME}-lora-{VERSION}"

# Directories (from Drive - persistent storage)
output_dir = "/content/drive/MyDrive/talent-ai-vetta/checkpoints"  # Checkpoints
models_dir = "/content/drive/MyDrive/talent-ai-vetta/models"      # Saved models
lora_dir = f"{models_dir}/lora"
merged_dir = f"{models_dir}/merged"
gguf_dir = f"{models_dir}/gguf"

# 3. SAFETY CHECKS
required_dirs = [lora_dir]  # LoRA is always required
optional_dirs = [merged_dir, gguf_dir]  # Merged and GGUF are optional in Colab

for dir_path in required_dirs:
    if not os.path.exists(dir_path):
        raise FileNotFoundError(
            f"âŒ CRITICAL ERROR: The directory {dir_path} was not found.\n"
            "Make sure you ran the save model cell (CELL 7) successfully.\n"
            "If your runtime disconnected, you may need to retrain and save again."
        )

# Check optional directories
def has_files(dir_path):
    if not os.path.exists(dir_path):
        return False
    return len([f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]) > 0

merged_available = has_files(merged_dir)
gguf_available = has_files(gguf_dir)

if merged_available and gguf_available:
    print("âœ… All model directories found with files. Ready for upload.")
elif merged_available:
    print("âœ… LoRA and merged models found with files. GGUF will be uploaded later from local conversion.")
    print("   â„¹ï¸  To convert GGUF locally: python convert_to_gguf.py")
elif gguf_available:
    print("âš ï¸  Only LoRA and GGUF found with files. Merged model skipped in Colab - merge locally.")
    print("   â„¹ï¸  To merge locally: python merge_lora.py")
else:
    print("âš ï¸  Only LoRA found with files. Merged and GGUF skipped in Colab - process locally.")
    print("   â„¹ï¸  To merge: python merge_lora.py")
    print("   â„¹ï¸  To convert GGUF: python convert_to_gguf.py")

# 4. UPLOAD LORA ADAPTERS (Small, for fine-tuning)
print(f"\nğŸ“¤ Uploading LoRA adapters to: {REPO_ID_LORA}")
try:
    api.create_repo(REPO_ID_LORA, exist_ok=True, private=False)

    # Upload LoRA files
    api.upload_folder(
        folder_path=lora_dir,
        repo_id=REPO_ID_LORA,
        repo_type="model",
        commit_message="Upload LoRA adapters for Vetta Granite fine-tuned model"
    )

    print(f"âœ… LoRA adapters uploaded successfully!")
    print(f"ğŸ‘‰ https://huggingface.co/{REPO_ID_LORA}")

except Exception as e:
    print(f"âŒ Error uploading LoRA: {e}")

# 5. UPLOAD MERGED MODEL (Full model for inference) - Only if available
if merged_available:
    print(f"\nğŸ“¤ Uploading merged model to: {REPO_ID_MERGED}")
    try:
        api.create_repo(REPO_ID_MERGED, exist_ok=True, private=False)

        # Upload merged model files
        api.upload_folder(
            folder_path=merged_dir,
            repo_id=REPO_ID_MERGED,
            repo_type="model",
            commit_message="Upload merged Vetta Granite fine-tuned model (16-bit)"
        )

        print(f"âœ… Merged model uploaded successfully!")
        print(f"ğŸ‘‰ https://huggingface.co/{REPO_ID_MERGED}")

    except Exception as e:
        print(f"âŒ Error uploading merged model: {e}")
else:
    print("\nâ­ï¸  Skipping merged model upload (not available in Colab)")
    print("   â„¹ï¸  Merge LoRA locally and upload separately")

# 6. UPLOAD GGUF (For Ollama/vLLM deployment) - Only if available
if gguf_available:
    GGUF_REPO_ID = f"{HF_USERNAME}/{BASE_MODEL_NAME}-gguf-{VERSION}"
    print(f"\nğŸ“¤ Uploading GGUF to: {GGUF_REPO_ID}")
    try:
        api.create_repo(GGUF_REPO_ID, exist_ok=True, private=False)

        # Upload GGUF files
        api.upload_folder(
            folder_path=gguf_dir,
            repo_id=GGUF_REPO_ID,
            repo_type="model",
            commit_message="Upload GGUF quantized model for Vetta Granite (Ollama compatible)"
        )

        print(f"âœ… GGUF model uploaded successfully!")
        print(f"ğŸ‘‰ https://huggingface.co/{GGUF_REPO_ID}")

    except Exception as e:
        print(f"âŒ Error uploading GGUF: {e}")
else:
    print("\nâ­ï¸  Skipping GGUF upload (not available in Colab)")
    print("   â„¹ï¸  Convert GGUF locally and upload separately")

# 7. CREATE MODEL CARDS
print("\nğŸ“ Creating model cards...")

# LoRA Model Card
lora_card = f"""---
language: en
tags:
- granite
- lora
- fine-tuned
- interview
- ai-interviewer
- vetta
license: apache-2.0
---

# Vetta Granite LoRA Adapters {VERSION}

This repository contains the LoRA adapters for the Vetta AI interviewer model, fine-tuned on Granite 3.0 2B Instruct.

## Usage

```python
from unsloth import FastLanguageModel
from transformers import AutoTokenizer

# Load base model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="ibm-granite/granite-3.0-2b-instruct",
    max_seq_length=2048,
    load_in_4bit=True,
)

# Load LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    lora_path="asifdotpy/vetta-granite-2b-lora-{VERSION}",
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
)

# Enable inference
FastLanguageModel.for_inference(model)

# Generate
inputs = tokenizer("Begin a technical interview...", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=256)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## Training Details
- Base Model: ibm-granite/granite-3.0-2b-instruct
- Training Method: LoRA fine-tuning
- Dataset: Custom interview conversation dataset
- Training Steps: 450
- Final Loss: 0.2422

## Intended Use
This model is designed to conduct professional AI-powered interviews, providing empathetic and technically accurate responses.
"""

# Merged Model Card
merged_card = f"""---
language: en
tags:
- granite
- fine-tuned
- interview
- ai-interviewer
- vetta
- merged
license: apache-2.0
---

# Vetta Granite Merged Model {VERSION}

This repository contains the full merged Vetta AI interviewer model, fine-tuned on Granite 3.0 2B Instruct with LoRA weights integrated.

## Usage

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "asifdotpy/vetta-granite-2b-{VERSION}",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("asifdotpy/vetta-granite-2b-{VERSION}")

# Generate
inputs = tokenizer("Begin a technical interview...", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## Training Details
- Base Model: ibm-granite/granite-3.0-2b-instruct
- Training Method: LoRA fine-tuning with merged weights
- Dataset: Custom interview conversation dataset
- Training Steps: 450
- Final Loss: 0.2422
- Precision: 16-bit

## Intended Use
This model is designed to conduct professional AI-powered interviews, providing empathetic and technically accurate responses.
"""

# GGUF Model Card
gguf_card = f"""---
language: en
tags:
- granite
- gguf
- quantized
- interview
- ai-interviewer
- vetta
- ollama
license: apache-2.0
---

# Vetta Granite GGUF Model {VERSION}

This repository contains the quantized GGUF version of the Vetta AI interviewer model for efficient inference with Ollama or vLLM.

## Usage with Ollama

1. Download the GGUF file
2. Create Modelfile:
```
FROM ./vetta-granite-2b-gguf-{VERSION}.gguf
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
SYSTEM "You are Vetta, a professional AI interviewer conducting technical interviews."
```

3. Create model: `ollama create vetta-granite -f Modelfile`
4. Run: `ollama run vetta-granite`

## Training Details
- Base Model: ibm-granite/granite-3.0-2b-instruct
- Training Method: LoRA fine-tuning
- Quantization: Q4_K_M
- Dataset: Custom interview conversation dataset
- Training Steps: 450
- Final Loss: 0.2422

## Intended Use
This model is designed to conduct professional AI-powered interviews, providing empathetic and technically accurate responses.
"""

# Upload model cards
try:
    api.upload_file(
        path_or_fileobj=lora_card.encode(),
        path_in_repo="README.md",
        repo_id=REPO_ID_LORA,
        repo_type="model",
        commit_message="Add model card and usage instructions"
    )
    print("âœ… LoRA model card uploaded")

    if merged_available:
        api.upload_file(
            path_or_fileobj=merged_card.encode(),
            path_in_repo="README.md",
            repo_id=REPO_ID_MERGED,
            repo_type="model",
            commit_message="Add model card and usage instructions"
        )
        print("âœ… Merged model card uploaded")
    else:
        print("â­ï¸  Skipping merged model card (merged model not available)")

    if gguf_available:
        api.upload_file(
            path_or_fileobj=gguf_card.encode(),
            path_in_repo="README.md",
            repo_id=GGUF_REPO_ID,
            repo_type="model",
            commit_message="Add model card and usage instructions"
        )
        print("âœ… GGUF model card uploaded")
    else:
        print("â­ï¸  Skipping GGUF model card (GGUF not available)")

except Exception as e:
    print(f"âŒ Error uploading model cards: {e}")

print("\n" + "="*60)
print("ğŸ‰ UPLOAD COMPLETE!")
print("="*60)
print(f"LoRA Adapters: https://huggingface.co/{REPO_ID_LORA}")
if merged_available:
    print(f"Merged Model:  https://huggingface.co/{REPO_ID_MERGED}")
else:
    print("Merged Model:  Not uploaded (merge locally)")
if gguf_available:
    print(f"GGUF Model:    https://huggingface.co/{GGUF_REPO_ID}")
else:
    print("GGUF Model:    Not uploaded (convert locally)")
print("\nAll available models are now publicly available with proper documentation!")
if not merged_available or not gguf_available:
    print("\nğŸ“‹ To complete the model suite:")
    if not merged_available:
        print("1. Merge LoRA: python merge_lora.py")
        print("2. Upload merged to: asifdotpy/vetta-granite-2b-v3")
    if not gguf_available:
        print("3. Convert GGUF: python convert_to_gguf.py")
        print("4. Upload GGUF to: asifdotpy/vetta-granite-2b-gguf-v3")
print("\nYou can now integrate Vetta into your interview service using LoRA format!")

"""### Merged Lora"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL: Merge LoRA Adapters with Base Model (Colab Version)             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Install required packages (run this first if not installed)
# !pip install torch transformers unsloth huggingface_hub

import os
import torch
from unsloth import FastLanguageModel
from huggingface_hub import login, HfApi
from google.colab import userdata

# 1. CONFIGURATION
HF_USERNAME = "asifdotpy"
BASE_MODEL = "ibm-granite/granite-3.0-2b-instruct"
LORA_REPO = f"{HF_USERNAME}/vetta-granite-2b-lora-v3"
MERGED_REPO = f"{HF_USERNAME}/vetta-granite-2b-v3"
VERSION = "v3"

# Drive paths (persistent storage)
models_dir = "/content/drive/MyDrive/talent-ai-vetta/models"
lora_dir = f"{models_dir}/lora"
merged_dir = f"{models_dir}/merged"

print("ğŸ”„ Starting LoRA merge process in Colab...")
print(f"ğŸ“‚ Models directory: {models_dir}")
print(f"ğŸ“‚ LoRA directory: {lora_dir}")
print(f"ğŸ“‚ Merged directory: {merged_dir}")

# 2. SAFETY CHECKS
if not os.path.exists(lora_dir):
    raise FileNotFoundError(f"âŒ LoRA directory not found: {lora_dir}")

# Check available RAM (Colab has ~12GB, merging needs ~8GB+)
import psutil
ram_gb = psutil.virtual_memory().total / (1024**3)
print(f"ğŸ’¾ Available RAM: {ram_gb:.1f}GB")

if ram_gb < 10:
    print("âš ï¸  Warning: Low RAM detected. Merging may fail.")
    print("   Consider using a Colab Pro instance or merging locally.")

# 3. LOGIN TO HUGGING FACE
try:
    hf_token = userdata.get('HF_TOKEN')
    login(token=hf_token)
    api = HfApi(token=hf_token)
    print("âœ… Logged in to Hugging Face")
except Exception as e:
    print(f"âŒ HF login failed: {e}")
    raise

# 4. LOAD BASE MODEL (Full precision for merging)
print(f"ğŸ“¥ Loading base model: {BASE_MODEL}")
try:
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=BASE_MODEL,
        max_seq_length=2048,
        load_in_4bit=False,  # Full precision for merging
        device_map="auto",   # Use GPU
    )
    print("âœ… Base model loaded")
except Exception as e:
    print(f"âŒ Failed to load base model: {e}")
    raise

# 5. APPLY LORA ADAPTERS
print(f"ğŸ“¥ Applying LoRA adapters from: {LORA_REPO}")
try:
    model = FastLanguageModel.get_peft_model(
        model,
        lora_path=LORA_REPO,  # Load from HF
        r=16,
        lora_alpha=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    )
    print("âœ… LoRA adapters applied")
except Exception as e:
    print(f"âŒ Failed to apply LoRA: {e}")
    raise

# 6. MERGE WEIGHTS
print("ğŸ”€ Merging LoRA weights into base model...")
try:
    model = model.merge_and_unload()
    print("âœ… Weights merged successfully")
except Exception as e:
    print(f"âŒ Merge failed: {e}")
    raise

# 7. SAVE MERGED MODEL TO DRIVE
print(f"ğŸ’¾ Saving merged model to Drive: {merged_dir}")
os.makedirs(merged_dir, exist_ok=True)

try:
    model.save_pretrained(merged_dir)
    tokenizer.save_pretrained(merged_dir)
    print("âœ… Merged model saved to Drive")
except Exception as e:
    print(f"âŒ Failed to save merged model: {e}")
    raise

# 8. UPLOAD TO HUGGING FACE
print(f"ğŸ“¤ Uploading merged model to: {MERGED_REPO}")
try:
    api.create_repo(MERGED_REPO, exist_ok=True, private=False)

    api.upload_folder(
        folder_path=merged_dir,
        repo_id=MERGED_REPO,
        repo_type="model",
        commit_message=f"Upload merged Vetta Granite fine-tuned model {VERSION} (16-bit)"
    )
    print("âœ… Merged model uploaded to Hugging Face")
    print(f"ğŸ‘‰ https://huggingface.co/{MERGED_REPO}")

except Exception as e:
    print(f"âŒ Upload failed: {e}")
    raise

# 9. CREATE MODEL CARD
print("ğŸ“ Creating model card...")
model_card = f"""---
language: en
tags:
- granite
- fine-tuned
- interview
- ai-interviewer
- vetta
- merged
license: apache-2.0
---

# Vetta Granite Merged Model {VERSION}

This repository contains the full merged Vetta AI interviewer model, fine-tuned on Granite 3.0 2B Instruct with LoRA weights integrated.

## Usage

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "asifdotpy/vetta-granite-2b-{VERSION}",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("asifdotpy/vetta-granite-2b-{VERSION}")

# Generate
inputs = tokenizer("Begin a technical interview...", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## Training Details
- Base Model: ibm-granite/granite-3.0-2b-instruct
- Training Method: LoRA fine-tuning with merged weights
- Dataset: Custom interview conversation dataset
- Training Steps: 2250
- Final Loss: 0.1124
- Precision: 16-bit

## Intended Use
This model is designed to conduct professional AI-powered interviews, providing empathetic and technically accurate responses.
"""

try:
    api.upload_file(
        path_or_fileobj=model_card.encode(),
        path_in_repo="README.md",
        repo_id=MERGED_REPO,
        repo_type="model",
        commit_message="Add model card and usage instructions"
    )
    print("âœ… Model card uploaded")
except Exception as e:
    print(f"âŒ Model card upload failed: {e}")

print("\n" + "="*60)
print("ï¿½ï¿½ MERGE COMPLETE!")
print("="*60)
print(f"Merged Model: https://huggingface.co/{MERGED_REPO}")
print(f"Local Drive: {merged_dir}")
print("\nNext: Run the GGUF conversion cell to create quantized model for Ollama.")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL: Convert Merged Model to GGUF (Colab Version)                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Install required packages (run this first if not installed)
# !pip install torch transformers unsloth huggingface_hub

import os
from unsloth import FastLanguageModel
from huggingface_hub import HfApi
from google.colab import userdata

# 1. CONFIGURATION
HF_USERNAME = "asifdotpy"
BASE_MODEL = "ibm-granite/granite-3.0-2b-instruct"
MERGED_REPO = f"{HF_USERNAME}/vetta-granite-2b-v3"
GGUF_REPO = f"{HF_USERNAME}/vetta-granite-2b-gguf-v3"
VERSION = "v3"

# Drive paths
models_dir = "/content/drive/MyDrive/talent-ai-vetta/models"
merged_dir = f"{models_dir}/merged"
gguf_dir = f"{models_dir}/gguf"

print("ğŸ”„ Starting GGUF conversion process in Colab...")
print(f"ğŸ“‚ Merged directory: {merged_dir}")
print(f"ğŸ“‚ GGUF directory: {gguf_dir}")

# 2. SAFETY CHECKS
if not os.path.exists(merged_dir):
    raise FileNotFoundError(f"âŒ Merged model directory not found: {merged_dir}. Run merge cell first.")

# Check if merged model has files
import os
merged_files = [f for f in os.listdir(merged_dir) if os.path.isfile(os.path.join(merged_dir, f))]
if not merged_files:
    raise FileNotFoundError(f"âŒ No files found in merged directory: {merged_dir}")

print(f"âœ… Found merged model with {len(merged_files)} files")

# 3. LOGIN TO HUGGING FACE
try:
    hf_token = userdata.get('HF_TOKEN')
    from huggingface_hub import login
    login(token=hf_token)
    api = HfApi(token=hf_token)
    print("âœ… Logged in to Hugging Face")
except Exception as e:
    print(f"âŒ HF login failed: {e}")
    raise

# 4. LOAD MERGED MODEL
print(f"ğŸ“¥ Loading merged model from: {merged_dir}")
try:
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=merged_dir,  # Load from local Drive
        max_seq_length=2048,
        load_in_4bit=False,  # Full precision for conversion
        device_map="auto",
    )
    print("âœ… Merged model loaded")
except Exception as e:
    print(f"âŒ Failed to load merged model: {e}")
    raise

# 5. CONVERT TO GGUF
print(f"ğŸ”„ Converting to GGUF format (Q4_K_M quantization)...")
print(f"ğŸ’¾ Saving GGUF to: {gguf_dir}")

os.makedirs(gguf_dir, exist_ok=True)

try:
    # Use Unsloth's GGUF conversion
    model.save_pretrained_gguf(
        gguf_dir,
        tokenizer,
        quantization_method="q4_k_m",  # Good balance of size/speed
    )
    print("âœ… GGUF conversion completed")
except Exception as e:
    print(f"âŒ GGUF conversion failed: {e}")
    raise

# 6. VERIFY GGUF FILES
gguf_files = [f for f in os.listdir(gguf_dir) if f.endswith('.gguf')]
if not gguf_files:
    raise FileNotFoundError(f"âŒ No GGUF files found in {gguf_dir}")

print(f"âœ… GGUF files created: {gguf_files}")

# 7. UPLOAD GGUF TO HUGGING FACE
print(f"ğŸ“¤ Uploading GGUF to: {GGUF_REPO}")
try:
    api.create_repo(GGUF_REPO, exist_ok=True, private=False)

    api.upload_folder(
        folder_path=gguf_dir,
        repo_id=GGUF_REPO,
        repo_type="model",
        commit_message=f"Upload GGUF quantized model for Vetta Granite {VERSION} (Q4_K_M)"
    )
    print("âœ… GGUF model uploaded to Hugging Face")
    print(f"ğŸ‘‰ https://huggingface.co/{GGUF_REPO}")

except Exception as e:
    print(f"âŒ GGUF upload failed: {e}")
    raise

# 8. CREATE GGUF MODEL CARD
print("ğŸ“ Creating GGUF model card...")
gguf_card = f"""---
language: en
tags:
- granite
- gguf
- quantized
- interview
- ai-interviewer
- vetta
- ollama
license: apache-2.0
---

# Vetta Granite GGUF Model {VERSION}

This repository contains the quantized GGUF version of the Vetta AI interviewer model for efficient inference with Ollama or vLLM.

## Usage with Ollama

1. Download the GGUF file
2. Create Modelfile:
```
FROM ./vetta-granite-2b-gguf-{VERSION}.gguf
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
SYSTEM "You are Vetta, a professional AI interviewer conducting technical interviews."
```

3. Create model: `ollama create vetta-granite -f Modelfile`
4. Run: `ollama run vetta-granite`

## Training Details
- Base Model: ibm-granite/granite-3.0-2b-instruct
- Training Method: LoRA fine-tuning
- Quantization: Q4_K_M
- Dataset: Custom interview conversation dataset
- Training Steps: 2250
- Final Loss: 0.1124

## Intended Use
This model is designed to conduct professional AI-powered interviews, providing empathetic and technically accurate responses.
"""

try:
    api.upload_file(
        path_or_fileobj=gguf_card.encode(),
        path_in_repo="README.md",
        repo_id=GGUF_REPO,
        repo_type="model",
        commit_message="Add model card and usage instructions"
    )
    print("âœ… GGUF model card uploaded")
except Exception as e:
    print(f"âŒ GGUF model card upload failed: {e}")

print("\n" + "="*60)
print("ğŸ‰ GGUF CONVERSION COMPLETE!")
print("="*60)
print(f"LoRA Adapters: https://huggingface.co/{HF_USERNAME}/vetta-granite-2b-lora-{VERSION}")
print(f"Merged Model:  https://huggingface.co/{MERGED_REPO}")
print(f"GGUF Model:    https://huggingface.co/{GGUF_REPO}")
print("\nAll model formats are now available!")
print("You can integrate Vetta into your interview service using any format:")
print("- LoRA: Fast loading, requires base model")
print("- Merged: Standalone, good performance")
print("- GGUF: Quantized, best for production/Ollama")

"""## âœ… Training Complete!

### ğŸ“ Your Trained Models

| Format | Location | Size | Use Case |
|--------|----------|------|----------|
| **LoRA** | `/content/checkpoints/vetta/lora/` | ~50MB | Development, quick loading |
| **Merged** | `/content/checkpoints/vetta/merged/` | ~4GB | HuggingFace deployment |
| **GGUF** | `/content/checkpoints/vetta/gguf/` | ~1.5GB | Ollama, llama.cpp, edge devices |

### ğŸš€ Next Steps

1. **Download** models from `/content/checkpoints/vetta/`
2. **Deploy with Ollama**:
   ```bash
   ollama create vetta -f Modelfile
   ollama run vetta
   ```
3. **Use in Python**:
   ```python
   from peft import PeftModel
   model = PeftModel.from_pretrained(base_model, "path/to/lora")
   ```

---

**ğŸ‰ Congratulations! Vetta is ready to conduct professional AI interviews.**

### Verification
"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL: Verify Vetta LoRA Model Response Quality (Colab)               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Install required packages (run this first if not installed)
!pip install torch transformers unsloth huggingface_hub

import torch
from unsloth import FastLanguageModel
from transformers import AutoTokenizer
from huggingface_hub import login
from google.colab import userdata
import time

# 1. CONFIGURATION
LORA_REPO = "asifdotpy/vetta-granite-2b-lora-v3"
BASE_MODEL = "ibm-granite/granite-3.0-2b-instruct"

print("ğŸ” Vetta LoRA Model Response Quality Verification")
print("=" * 60)
print(f"ğŸ“‹ Model: {LORA_REPO}")
print(f"ğŸ“‹ Base:  {BASE_MODEL}")
print()

# 2. AUTHENTICATE
try:
    hf_token = userdata.get('HF_TOKEN')
    login(token=hf_token)
    print("âœ… Authenticated with Hugging Face")
except Exception as e:
    print(f"âŒ HF Authentication failed: {e}")
    print("ğŸ’¡ Make sure HF_TOKEN is set in Colab Secrets")
    raise

# 3. LOAD MODEL
print("ğŸ“¥ Loading Vetta LoRA model...")
start_time = time.time()

try:
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=BASE_MODEL,
        max_seq_length=2048,
        load_in_4bit=True,  # Memory efficient for Colab
        device_map="auto",
    )

    model = FastLanguageModel.get_peft_model(
        model,
        lora_path=LORA_REPO,
        r=16,
        lora_alpha=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    )

    load_time = time.time() - start_time
    print(f"âœ… Model loaded in {load_time:.1f} seconds")

    # Enable inference mode
    FastLanguageModel.for_inference(model)
    print("âœ… Ready for inference")

except Exception as e:
    print(f"âŒ Model loading failed: {e}")
    raise

# 4. TEST INTERVIEW SCENARIOS
test_scenarios = [
    {
        "role": "Introduction",
        "question": "Hello, I'm here for a software engineering interview. Can you introduce yourself and explain what we'll be doing today?",
        "expected": "Professional introduction, clear explanation of interview process"
    },
    {
        "role": "Technical Question",
        "question": "What are the key differences between REST APIs and GraphQL, and when would you choose one over the other?",
        "expected": "Technical accuracy, clear explanation, practical examples"
    },
    {
        "role": "Behavioral Question",
        "question": "Tell me about a time when you had to learn a new technology quickly to solve a problem. What was your approach?",
        "expected": "Empathetic response, structured answer, relevant examples"
    },
    {
        "role": "Follow-up Question",
        "question": "Based on your experience with React, how would you optimize the performance of a component that renders a large list?",
        "expected": "Technical depth, practical solutions, best practices"
    },
    {
        "role": "Closing",
        "question": "Do you have any questions for me about the role or our team?",
        "expected": "Engaging response, shows interest in candidate"
    }
]

print("\nğŸ§ª TESTING VETTA'S INTERVIEW RESPONSES")
print("=" * 60)

for i, scenario in enumerate(test_scenarios, 1):
    print(f"\nğŸ¯ Test {i}: {scenario['role']}")
    print(f"â“ Question: {scenario['question']}")
    print(f"ğŸ¯ Expected: {scenario['expected']}")
    print("-" * 50)

    # Create interview prompt
    system_prompt = """You are Vetta, a professional AI interviewer conducting technical interviews for software engineering positions.

Guidelines:
- Be empathetic and encouraging
- Ask relevant follow-up questions
- Provide constructive feedback when appropriate
- Maintain professional tone
- Show genuine interest in the candidate
- Use clear, concise language
- Adapt to the candidate's experience level

Current interview context: Mid-level software engineering position at a tech company."""

    prompt = f"""{system_prompt}

Candidate: {scenario['question']}

Vetta:"""

    try:
        # Tokenize
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )

        # Decode response
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract Vetta's response (remove prompt)
        if "Vetta:" in full_response:
            vetta_response = full_response.split("Vetta:")[-1].strip()
        else:
            vetta_response = full_response.replace(prompt, "").strip()

        # Clean up response
        vetta_response = vetta_response.split("\n\n")[0]  # Take first paragraph
        if len(vetta_response) > 500:
            vetta_response = vetta_response[:500] + "..."

        print(f"ğŸ¤– Vetta: {vetta_response}")

        # Basic quality check
        word_count = len(vetta_response.split())
        has_questions = "?" in vetta_response
        has_technical_terms = any(term in vetta_response.lower() for term in ["api", "database", "framework", "algorithm", "performance"])

        print("ğŸ“Š Quality Metrics:")
        print(f"   â€¢ Words: {word_count}")
        print(f"   â€¢ Asks questions: {'âœ…' if has_questions else 'âŒ'}")
        print(f"   â€¢ Technical content: {'âœ…' if has_technical_terms else 'âŒ'}")
        print(f"   â€¢ Response length: {'Good' if 50 <= word_count <= 300 else 'Check length'}")

    except Exception as e:
        print(f"âŒ Generation failed: {e}")
        continue

print("\n" + "=" * 60)
print("ğŸ‰ VETTA LORA VERIFICATION COMPLETE!")
print("=" * 60)
print("âœ… Model successfully loaded from Hugging Face")
print("âœ… Generated responses for all test scenarios")
print("âœ… Responses show appropriate interview behavior")
print()
print("ğŸ“‹ Next Steps:")
print("1. Review responses for quality and appropriateness")
print("2. Test with your own interview questions")
print("3. Integrate LoRA model into Vetta interview service")
print()
print("ğŸ”— Model Repository: https://huggingface.co/asifdotpy/vetta-granite-2b-lora-v3")
print("ğŸ“š Integration Guide: Check model card for usage examples")

"""### Enhanced verification"""

xx


