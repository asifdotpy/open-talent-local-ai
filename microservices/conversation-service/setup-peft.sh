#!/bin/bash
# Lightweight setup script for PEFT/LoRA model integration in conversation service

set -e

echo "ğŸ”„ Setting up PEFT/LoRA model integration for TalentAI Conversation Service..."
echo "ğŸ“ Note: Heavy computation (model packaging) should be done on Colab first!"

# Check if we're in the right directory
if [ ! -f "requirements.txt" ] || [ ! -d "app" ]; then
    echo "âŒ Please run this script from the conversation-service directory"
    exit 1
fi

# Install lightweight dependencies (no heavy model downloads)
echo "ğŸ“¦ Installing lightweight PEFT dependencies..."
pip install torch transformers peft accelerate

# Create .env file if it doesn't exist
if [ ! -f ".env" ]; then
    echo "ğŸ“ Creating .env file from template..."
    cp .env.example .env
    echo "âœ… Created .env file. Please edit it with your configuration."
else
    echo "â„¹ï¸  .env file already exists"
fi

# Test basic PEFT imports (no model loading)
echo "ğŸ§ª Testing PEFT imports..."
python -c "
import sys
sys.path.append('.')

try:
    # Test imports
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel, PeftConfig
    from app.services.modular_llm_service import LLMProvider, LLMConfig
    
    print('âœ… All PEFT dependencies imported successfully')
    print('âœ… Modular LLM service imports working')
    
except ImportError as e:
    print(f'âŒ Import failed: {e}')
    sys.exit(1)
"

echo ""
echo "ğŸ‰ Lightweight PEFT setup complete!"
echo ""
echo "ğŸ“‹ IMPORTANT: Before running the service, you need to:"
echo ""
echo "1. ğŸš€ Run the Colab notebook to package the model:"
echo "   https://colab.research.google.com/notebook#fileId=package_peft_model_colab.ipynb"
echo ""
echo "2. ğŸ“ Update .env file:"
echo "   LLM_PROVIDER=peft"
echo "   LLM_MODEL=asifdotpy/vetta-granite-2b-packaged-v3"
echo ""
echo "3. â–¶ï¸  Start the service:"
echo "   python -m uvicorn app.main:app --host 0.0.0.0 --port 8003"
echo ""
echo "4. ğŸ§ª Test the endpoint:"
echo "   curl -X POST http://localhost:8003/api/v1/interviews/generate-questions \\"
echo "        -H 'Content-Type: application/json' \\"
echo "        -d '{\"job_description\": \"Python developer\", \"num_questions\": 3}'"
echo ""
echo "ğŸ’¡ For multiple personas: Create different packaged models and switch LLM_MODEL"